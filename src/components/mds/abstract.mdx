# Abstract

The integration of large language models (LLMs) into robotic systems has witnessed significant growth, where LLMs can convert natural language instructions into executable robot policies (e.g., *grasp()*, *move()*). However, the inherent vulnerability of LLMs to jailbreak attacks - bypassing ethical safeguards to generate hazardous, violent, or discriminatory content - presents critical security risks and thus has been extensively studied. Whereas traditional LLM jailbreak research focuses on the digital domain, physical-world robotic systems introduce catastrophic risk amplification through embodied executors. A compromised LLM-based robot could execute destructive kinematic actions and cause physical harm that transcends the textual/digital harms of traditional attacks.

In this paper, we investigate the feasibility and rationale of jailbreak attacks against LLM-based robots and answer three research questions: (1) How applicable are existing LLM jailbreak attacks against LLM-based robots? (2) What unique challenges arise if they are not directly applicable? (3) How to defend against such jailbreak attacks?

To this end, we first construct a "human-object-environment" robot risks-oriented Harmful-RLbench and then conduct a measurement study on LLM-based robot systems. Our findings conclude that traditional LLM jailbreak attacks are inapplicable in robot scenarios, and we identify two unique challenges: determining policy-executable optimization directions and accurately evaluating robot-jailbroken policies.

To enable a more thorough security analysis, we introduce **POEX** (**PO**licy **EX**ecutable) jailbreak, a red-teaming framework that induces harmful yet executable policy to jailbreak LLM-based robots. POEX incorporates hidden layer gradient optimization to guarantee jailbreak success and policy execution as well as a multi-agent evaluator to accurately assess the practical executability of policies. 

Experiments conducted on the real-world robotic systems (robotic arms, humanoid robots) and in simulation demonstrate POEX's efficacy, highlighting critical security vulnerabilities and its transferability across LLMs. Finally, we propose prompt-based and model-based defenses to mitigate attacks. Our findings underscore the urgent need for security measures to ensure the safe deployment of LLM-based robots in critical applications.